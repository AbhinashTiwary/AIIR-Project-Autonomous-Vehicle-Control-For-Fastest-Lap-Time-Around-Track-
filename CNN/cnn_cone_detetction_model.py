# -*- coding: utf-8 -*-
"""CNN_Cone_Detetction_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oacs76daebNZRde_-qfa8B2qgOpQmM9j
"""

!git clone https://github.com/AbhinashTiwary/Data_DL.git

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline



"""
Tools for ploting"""

import torch
import numpy as np
import matplotlib.pyplot as plt

import torchvision.transforms.functional as F


plt.rcParams["savefig.bbox"] = 'tight'


def show(imgs):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = F.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

"""And load images"""

from torchvision.utils import make_grid
from torchvision.io import read_image
from pathlib import Path

# Define the paths for the images
path_red_cone = Path('Data_DL/red_cone_image.jpg')
path_blue_cone = Path('Data_DL/blue_cone_image.jpg')

# Check if the files exist
if not path_red_cone.is_file():
    print(f"Error: {path_red_cone} does not exist")
else:
    red_cone_image = read_image(str(path_red_cone))
    print(red_cone_image.shape)  # Print the dimensions if the file is found

if not path_blue_cone.is_file():
    print(f"Error: {path_blue_cone} does not exist")
else:
    blue_cone_image = read_image(str(path_blue_cone))
    print(blue_cone_image.shape)  # Print the dimensions if the file is found

# Assuming both files exist and are correctly loaded
if path_red_cone.is_file() and path_blue_cone.is_file():
    image_list = [red_cone_image, blue_cone_image]
    show(image_list[0])
    show(image_list[1])

"""## Bounding boxes detection
A bounding box is a rectangle that specifies an object position, its class (e.g., a person or a dog) and confidence(how likely it is to be at that location). Bounding boxes are mainly used in the task of object detection, where the aim is identifying the position and type of multiple objects in the image.

We can use the function [draw_bounding_boxes](https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html) to draw bounding boxes on an image.


"""

from torchvision.utils import draw_bounding_boxes


boxes = torch.tensor([[50, 145, 780, 1250]], dtype=torch.float) # format is (xmin, ymin, xmax, ymax)
colors = ["yellow"]
result = draw_bounding_boxes(red_cone_image, boxes, colors=colors, width=5)
show(result)

"""Now, using a trained CNN model (here we are using [torchvision.models.detection.fasterrcnn_resnet50_fpn](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html)) to predict the bounding boxes.


"""

from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights


weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
transforms = weights.transforms()

inputs = [transforms(d) for d in image_list] # why do we need to transform the images? What is happening during the transformation?

model = fasterrcnn_resnet50_fpn(weights=weights, progress=False) # why do we need to load the weights?
model = model.eval() # what does this do?

outputs = model(inputs)

# What is the shape of the inputs tensors?
print(inputs[0].shape)

# What is the range of the inputs (max/min values)?
print(inputs[0].min(), inputs[0].max())

# What are the outputs predictions and how are they packed?
#print(outputs)

"""Let's plot the boxes detected by our model. We will only plot the boxes with a
score greater than a given threshold.


"""

# plot the predicted bounding boxes (hint, you can use enumerate)
for count, image in enumerate(image_list):
    boxes = outputs[count]['boxes']
    show(draw_bounding_boxes(image, boxes, width=4))

# Use a threshold for filtering the predictions
score_threshold = .7
for count, image in enumerate(image_list):
    boxes = outputs[count]['boxes'][outputs[count]['scores'] > score_threshold]
    show(draw_bounding_boxes(image, boxes, width=4))

"""## Semantic segmentation

Semantic segmentation is a deep learning algorithm that associates a label or category with every pixel in an image. It is used to recognize a collection of pixels that form distinct categories. For example, an autonomous vehicle needs to identify vehicles, pedestrians, traffic signs, pavement, and other road features.

The [torchvision.utils.draw_segmentation_masks](https://pytorch.org/vision/main/generated/torchvision.utils.draw_segmentation_masks.html) function can be used to plots those masks on top of the original image. This function expects the masks to be boolean masks, but our masks above contain probabilities in ``[0, 1]``. To get boolean masks, we can do the following:
"""

from torchvision.utils import draw_segmentation_masks
from torchvision.transforms.functional import resize

# Function to resize and convert images to uint8
def resize_and_convert(img, target_size):
    img_resized = resize(img, target_size)
    if img_resized.dtype != torch.uint8:
        img_resized = (img_resized * 255).to(dtype=torch.uint8)
    return img_resized

# Load the manual annotations
manual_annotation_red_cone = read_image('Data_DL/red_cone_annotation.png')
manual_annotation_blue_cone = read_image('Data_DL/blue_cone_annotation.png')

# Find the maximum dimensions across all images
max_height = max(manual_annotation_red_cone.shape[1], manual_annotation_blue_cone.shape[1], image_list[0].shape[1])
max_width = max(manual_annotation_red_cone.shape[2], manual_annotation_blue_cone.shape[2], image_list[0].shape[2])

# Resize the annotations
manual_annotation_red_cone_resized = resize_and_convert(manual_annotation_red_cone, (max_height, max_width))
manual_annotation_blue_cone_resized = resize_and_convert(manual_annotation_blue_cone, (max_height, max_width))

# Process the annotations
annotations = torch.cat(((manual_annotation_red_cone_resized[1] > 125).unsqueeze(0),
                         (manual_annotation_blue_cone_resized[1] > 125).unsqueeze(0)), dim=0)

# Resize the input image
image_to_show = resize_and_convert(image_list[0], (max_height, max_width))

# Visualize the annotations
show(draw_segmentation_masks(image_to_show, masks=annotations, alpha=0.7))
